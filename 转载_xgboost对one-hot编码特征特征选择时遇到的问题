转自：https://www.deeplearn.me/1625.html
xgboost对one-hot编码特征特征选择时遇到的问题

xgboost 对所有的输入特征都是当做数值型对待，所以你给定的数据也要是指定的数据类型
对于数据缺失或者稀疏，xgboost 都可以自己处理
纠结于 one-hot 编码问题主要是将分类信息转化为一定长度索引的二进制数据
假设当前的数据类型是 annimal={‘panda’,’cat’,’dog’}
经过 one-hot 编码可能就变成
[[1,0,0],
[0,1,0],
[0,0,1]]
上述是一个 3*3 矩阵向量
对于 xgboost 而言，将其解释为 3 个特征变量，animal0,animal1,animal2，这三个共同表征 animal
最终在 get_fscore 函数中计算特征的重要性也会将其分开来看，可能 animal0 占据着更重要的地位
xgboost 树模型其实是不建议使用 one-hot 编码，在 xgboost 上面的 issue 也提到过，相关的说明如下
I do not know what you mean by vector. xgboost treat every input feature as numerical, with support for missing values and sparsity. The decision is at the user
So if you want ordered variables, you can transform the variables into numerical levels(say age). Or if you prefer treat it as categorical variable, do one hot encoding.
在另一个issues上也提到过（tqchen commented on 8 May 2015）：
One-hot encoding could be helpful when the number of categories are small( in level of 10 to 100). In such case one-hot encoding can discover interesting interactions like (gender=male) AND (job = teacher).
While ordering them makes it harder to be discovered(need two split on job). However, indeed there is not a unified way handling categorical features in trees, and usually what tree was really good at was ordered continuous features anyway..
总结起来的结论，大至两条：
1.对于类别有序的类别型变量，比如 age 等，当成数值型变量处理可以的。对于非类别有序的类别型变量，推荐 one-hot。但是 one-hot 会增加内存开销以及训练时间开销。
2.类别型变量在范围较小时（tqchen 给出的是[10,100]范围内）推荐使用
为什么基于树模型不太需要 one-hot
one-hot 编码为什么可以解决类别型数据的离散值问题

首先，one-hot 编码是 N 位状态寄存器为 N 个状态进行编码的方式
eg：高、中、低不可分，→ 用 0 0 0 三位编码之后变得可分了，并且成为互相独立的事件
→ 类似 SVM 中，原本线性不可分的特征，经过 project 之后到高维之后变得可分了
GBDT 处理高维稀疏矩阵的时候效果并不好，即使是低维的稀疏矩阵也未必比 SVM 好
Tree Model 不太需要 one-hot 编码
对于决策树来说，one-hot 的本质是增加树的深度
tree-model 是在动态的过程中生成类似 One-Hot + Feature Crossing 的机制
1. 一个特征或者多个特征最终转换成一个叶子节点作为编码 ，one-hot 可以理解成三个独立事件
2. 决策树是没有特征大小的概念的，只有特征处于他分布的哪一部分的概念
one-hot 可以解决线性可分问题 但是比不上 label econding
one-hot 降维后的缺点：
降维前可以交叉的降维后可能变得不能交叉
树模型的训练过程：
从根节点到叶子节点整条路中有多少个节点相当于交叉了多少次，所以树的模型是自行交叉
eg：是否是长的 { 否（是→ 柚子，否 → 苹果） ，是 → 香蕉 } 园 cross 黄 → 形状 （圆，长） 颜色 （黄，红） one-hot 度为 4 的样本
使用树模型的叶子节点作为特征集交叉结果可以减少不必要的特征交叉的操作 或者减少维度和 degree 候选集
eg 2 degree → 8 的特征向量 树 → 3 个叶子节点
树模型：Ont-Hot + 高 degree 笛卡尔积 + lasso 要消耗更少的计算量和计算资源
这就是为什么树模型之后可以 stack 线性模型
n*m 的输入样本 → 决策树训练之后可以知道在哪一个叶子节点上 → 输出叶子节点的 index → 变成一个 n*1 的矩阵 → one-hot 编码 → 可以得到一个 n*o 的矩阵（o 是叶子节点的个数） → 训练一个线性模型
典型的使用： GBDT +　ＲＦ
优点 ： 节省做特征交叉的时间和空间
如果只使用 one-hot 训练模型，特征之间是独立的
对于现有模型的理解：（G（l（张量）））：
其中：l（·）为节点的模型
G（·）为节点的拓扑方式
神经网络：l（·）取逻辑回归模型
G（·）取全连接的方式
决策树： l（·）取 LR
G（·）取树形链接方式
创新点： l（·）取 NB，SVM 单层 NN ，等
G（·）取怎样的信息传递方式
参考资料
http://blog.csdn.net/pipisorry/article/details/61193868
http://d0evi1.com/onehot/
http://xgboost.readthedocs.io/en/latest/R-package/discoverYourData.html
